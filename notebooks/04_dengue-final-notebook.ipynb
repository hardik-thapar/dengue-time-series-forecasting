{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14792048,"sourceType":"datasetVersion","datasetId":9456914}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Dengue Time-Series Forecasting — Final Model\n\nThis notebook contains the final forecasting pipeline for predicting weekly dengue cases in San Juan and Iquitos.\n\nThe model combines climate variables with carefully engineered time-based features.  \nValidation is performed using strict chronological splits to reflect real-world forecasting.","metadata":{}},{"cell_type":"markdown","source":"## 1. Import Required Libraries\n\nWe import the libraries needed for:\n\n- Data handling (NumPy, Pandas)\n- Time-series cross-validation (TimeSeriesSplit)\n- Evaluation (MAE)\n- Modeling (CatBoost)\n\nThese form the core components of the forecasting pipeline.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_absolute_error\nfrom catboost import CatBoostRegressor\n\nprint(\"Libraries imported successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:49:29.619566Z","iopub.execute_input":"2026-02-11T16:49:29.619898Z","iopub.status.idle":"2026-02-11T16:49:35.195037Z","shell.execute_reply.started":"2026-02-11T16:49:29.619862Z","shell.execute_reply":"2026-02-11T16:49:35.193561Z"}},"outputs":[{"name":"stdout","text":"Libraries imported successfully!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## 2. Load Training Data\n\nWe load the DengAI training features and labels.\n\nThe dataset is indexed by:\n\n- City  \n- Year  \n- Week of year  \n\nKeeping this structure ensures that temporal ordering is preserved throughout the workflow.","metadata":{}},{"cell_type":"code","source":"# Load training data with proper time-series indexing\ntrain_features = pd.read_csv(\n    \"/kaggle/input/datasets/hardikthapar/deng123/DengAI_Training_Data_Features.csv\",\n    index_col=[0, 1, 2]  # city, year, weekofyear\n)\n\ntrain_labels = pd.read_csv(\n    \"/kaggle/input/datasets/hardikthapar/deng123/DengAI_Training_Data_Labels.csv\",\n    index_col=[0, 1, 2]\n)\n\nprint(f\"Training features shape: {train_features.shape}\")\nprint(f\"Training labels shape: {train_labels.shape}\")\nprint(f\"\\nCities: {train_features.index.get_level_values(0).unique().tolist()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:49:35.197328Z","iopub.execute_input":"2026-02-11T16:49:35.197822Z","iopub.status.idle":"2026-02-11T16:49:35.289927Z","shell.execute_reply.started":"2026-02-11T16:49:35.197794Z","shell.execute_reply":"2026-02-11T16:49:35.288863Z"}},"outputs":[{"name":"stdout","text":"Training features shape: (1456, 21)\nTraining labels shape: (1456, 1)\n\nCities: ['sj', 'iq']\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## 3. Separate Cities and Maintain Chronology\n\nSan Juan and Iquitos are modeled separately.\n\nEach city has different climate patterns and outbreak behavior, so separate models allow better specialization.\n\nWe also:\n\n- Remove the `week_start_date` column  \n- Sort all data chronologically  \n\nThis guarantees proper time ordering before feature engineering.","metadata":{}},{"cell_type":"code","source":"# Separate cities\nsj_X_raw = train_features.loc[\"sj\"].copy()\niq_X_raw = train_features.loc[\"iq\"].copy()\n\nsj_y = train_labels.loc[\"sj\"][\"total_cases\"].copy()\niq_y = train_labels.loc[\"iq\"][\"total_cases\"].copy()\n\n# Drop date column (already in index)\nsj_X_raw = sj_X_raw.drop(\"week_start_date\", axis=1, errors='ignore')\niq_X_raw = iq_X_raw.drop(\"week_start_date\", axis=1, errors='ignore')\n\n# Ensure chronological order\nsj_X_raw = sj_X_raw.sort_index()\niq_X_raw = iq_X_raw.sort_index()\nsj_y = sj_y.sort_index()\niq_y = iq_y.sort_index()\n\nprint(f\"San Juan: {len(sj_X_raw)} weeks\")\nprint(f\"Iquitos: {len(iq_X_raw)} weeks\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:49:35.291122Z","iopub.execute_input":"2026-02-11T16:49:35.291372Z","iopub.status.idle":"2026-02-11T16:49:35.319438Z","shell.execute_reply.started":"2026-02-11T16:49:35.291350Z","shell.execute_reply":"2026-02-11T16:49:35.318315Z"}},"outputs":[{"name":"stdout","text":"San Juan: 936 weeks\nIquitos: 520 weeks\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## 4. Select Core Climate Features\n\nFrom exploratory analysis, five climate variables were selected:\n\n- Specific humidity  \n- Dew point temperature  \n- Average temperature  \n- Minimum temperature  \n- Precipitation  \n\nThese are biologically relevant for mosquito growth and dengue transmission.\n\nLimiting features keeps the model focused and reduces unnecessary complexity.","metadata":{}},{"cell_type":"code","source":"# Select core climate features\nbase_features = [\n    \"reanalysis_specific_humidity_g_per_kg\",\n    \"reanalysis_dew_point_temp_k\",\n    \"station_avg_temp_c\",\n    \"station_min_temp_c\",\n    \"precipitation_amt_mm\"\n]\n\n# Extract base features\nsj_X_climate = sj_X_raw[base_features].copy()\niq_X_climate = iq_X_raw[base_features].copy()\n\nprint(\"Selected climate features:\")\nprint(base_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T17:12:57.907915Z","iopub.execute_input":"2026-02-11T17:12:57.908824Z","iopub.status.idle":"2026-02-11T17:12:57.917287Z","shell.execute_reply.started":"2026-02-11T17:12:57.908796Z","shell.execute_reply":"2026-02-11T17:12:57.916230Z"}},"outputs":[{"name":"stdout","text":"Selected climate features:\n['reanalysis_specific_humidity_g_per_kg', 'reanalysis_dew_point_temp_k', 'station_avg_temp_c', 'station_min_temp_c', 'precipitation_amt_mm']\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"## 5. Handle Missing Values\n\nClimate data occasionally contains gaps.\n\nWe apply forward-fill to maintain continuity in the time series without introducing future information.\n\nThis step is performed carefully to avoid data leakage.\n2. In production, historical observations would be available\n3. TimeSeriesSplit's expanding window ensures test data from earlier folds becomes training data in later folds\n\nRows with NaN values (first 12 weeks) are dropped as insufficient history exists.","metadata":{}},{"cell_type":"code","source":"def fill_missing_values(X_train, X_test):\n    \"\"\"\n    Fill missing values using forward-fill method.\n    Fit on train, apply to both train and test.\n    \"\"\"\n    X_train_filled = X_train.ffill().bfill()\n    X_test_filled = X_test.ffill().bfill()\n    \n    return X_train_filled, X_test_filled","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:49:35.335374Z","iopub.execute_input":"2026-02-11T16:49:35.335844Z","iopub.status.idle":"2026-02-11T16:49:35.355312Z","shell.execute_reply.started":"2026-02-11T16:49:35.335815Z","shell.execute_reply":"2026-02-11T16:49:35.353649Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## 6. Create Lag-Based Target Features\n\nDengue outbreaks show temporal dependency.\n\nTo capture this, we add:\n\n- Lag features (1–4 weeks)\n- Rolling averages (4 and 8 weeks)\n- Rolling standard deviation (4 weeks)\n\nAll lag features are strictly backward-looking using `shift(1)`.\n\nNo future values are used at any point.","metadata":{}},{"cell_type":"code","source":"def add_target_lags(X, y, lags=[1, 2, 3, 4], rolling_windows=[4, 8]):\n    \"\"\"\n    Add lagged target features without data leakage.\n    \n    Parameters:\n    X : Features matrix\n    y : Target variable (aligned with X ind)\n    lags : Individual lag periods\n    rolling_windows : Rolling mean window sizes\n    \n    Return:\n    DataFrame with lag features added\n    \"\"\"\n    X_lagged = X.copy()\n    \n    # Add individual lags\n    for lag in lags:\n        X_lagged[f'lag_{lag}'] = y.shift(lag)\n    \n    # Add rolling means (computed on shifted data to prevent leakage)\n    for window in rolling_windows:\n        X_lagged[f'rolling_mean_{window}'] = y.shift(1).rolling(window, min_periods=1).mean()\n    \n    # Add rolling std for variance information\n    X_lagged['rolling_std_4'] = y.shift(1).rolling(4, min_periods=1).std()\n    \n    return X_lagged","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:49:35.357124Z","iopub.execute_input":"2026-02-11T16:49:35.357547Z","iopub.status.idle":"2026-02-11T16:49:35.381888Z","shell.execute_reply.started":"2026-02-11T16:49:35.357506Z","shell.execute_reply":"2026-02-11T16:49:35.380677Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## 7. Time-Series based Cross-Validation\n\nWe use `TimeSeriesSplit` with 5 folds.\n\nFor each fold:\n\n1. Data is split chronologically  \n2. Missing values are handled per fold  \n3. Lag features are computed using only training data  \n4. The model is trained on past data and evaluated on future weeks  \n\nThis setup mirrors real forecasting conditions.\n\n#### Log-Transformed CatBoost Regression\n\nDengue case counts are highly right-skewed with occasional large outbreaks.\n\nWe apply log1p transformation to the target:\n- Training: y_log = log(1 + y)\n- Prediction: y_pred = exp(y_log) - 1\n\nThis stabilizes variance and helps the model learn patterns across different outbreak scales.\n\nCatBoost is configured with:\n- MAE loss function (aligned with evaluation metric)\n- 500 iterations with early stopping potential\n- Depth 6 and learning rate 0.03 (moderate complexity)\n- Random seed for reproducibility","metadata":{}},{"cell_type":"code","source":"def run_time_aware_cv(X_climate, y, city_name, n_splits=5, params=None):\n    \"\"\"\n    Perform time-series cross-validation with proper fold-wise feature engineering.\n    \n    This ensures that no data leaks due to:\n    1. Splitting data first\n    2. Computing lag features per-fold using only training data\n    3. Never shuffling data\n    \"\"\"\n    \n    if params is None:\n        params = {\n            'loss_function': 'MAE',\n            'iterations': 500,\n            'depth': 6,\n            'learning_rate': 0.03,\n            'random_seed': 42,\n            'verbose': 0\n        }\n    \n    tscv = TimeSeriesSplit(n_splits=n_splits)\n    fold_scores = []\n    \n\n    print(f\"{city_name} - Time Series Cross-Validation\")\n   \n    \n    for fold, (train_idx, test_idx) in enumerate(tscv.split(X_climate)):\n        \n        # Split climate features\n        X_train_climate = X_climate.iloc[train_idx]\n        X_test_climate = X_climate.iloc[test_idx]\n        \n        # Split target\n        y_train = y.iloc[train_idx]\n        y_test = y.iloc[test_idx]\n        \n        # Fill missing values per-fold\n        X_train_climate, X_test_climate = fill_missing_values(X_train_climate, X_test_climate)\n        \n        # Add lag features (CRITICAL: use full y for test to have historical context)\n        X_train_lagged = add_target_lags(X_train_climate, y_train)\n        X_test_lagged = add_target_lags(X_test_climate, y)  # Use full y series for lags\n        \n        # Drop rows with NaN (created from lag creation)\n        train_valid_idx = X_train_lagged.dropna().index\n        test_valid_idx = X_test_lagged.dropna().index\n        \n        X_train_final = X_train_lagged.loc[train_valid_idx]\n        y_train_final = y_train.loc[train_valid_idx]\n        \n        X_test_final = X_test_lagged.loc[test_valid_idx]\n        y_test_final = y_test.loc[test_valid_idx]\n        \n        # Log transform target (to stabilize variance)\n        y_train_log = np.log1p(y_train_final)\n        \n        # Train model\n        model = CatBoostRegressor(**params)\n        model.fit(X_train_final, y_train_log)\n        \n        # Predict and inverse transform\n        preds_log = model.predict(X_test_final)\n        preds = np.expm1(preds_log).clip(0)  # Ensure non-negative\n        \n        # Evaluate\n        fold_mae = mean_absolute_error(y_test_final, preds)\n        fold_scores.append(fold_mae)\n        \n        print(f\"Fold {fold + 1}/{n_splits} | \"\n              f\"Train size: {len(X_train_final):4d} | \"\n              f\"Test size: {len(X_test_final):3d} | \"\n              f\"MAE: {fold_mae:.4f}\")\n    \n    # Summary statistics\n    mean_mae = np.mean(fold_scores)\n    std_mae = np.std(fold_scores)\n    \n    print(f\"CV Results: Mean MAE = {mean_mae:.4f} ± {std_mae:.4f}\")\n    \n    return fold_scores, mean_mae","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T17:11:53.098628Z","iopub.execute_input":"2026-02-11T17:11:53.098936Z","iopub.status.idle":"2026-02-11T17:11:53.110771Z","shell.execute_reply.started":"2026-02-11T17:11:53.098914Z","shell.execute_reply":"2026-02-11T17:11:53.109295Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"## 8.  Cross-Validation \n#### San Juan\n\nThe full time-aware validation pipeline is executed for San Juan.\n\nMAE is computed for each fold and summarized using mean and standard deviation.\n\nThis provides an honest estimate of generalization performance.\n\n#### Iquitos\n\nThe same validation procedure is applied to Iquitos.\n\nSeparate validation ensures the model adapts correctly to the city's unique outbreak dynamics.","metadata":{}},{"cell_type":"code","source":"# San Juan CV\nsj_cv_scores, sj_mean_mae = run_time_aware_cv(\n    sj_X_climate, \n    sj_y, \n    \"San Juan\",\n    n_splits=5\n)\n\n# Iquitos CV\niq_cv_scores, iq_mean_mae = run_time_aware_cv(\n    iq_X_climate, \n    iq_y, \n    \"Iquitos\",\n    n_splits=5\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T17:11:57.219424Z","iopub.execute_input":"2026-02-11T17:11:57.219756Z","iopub.status.idle":"2026-02-11T17:12:04.378070Z","shell.execute_reply.started":"2026-02-11T17:11:57.219733Z","shell.execute_reply":"2026-02-11T17:12:04.376841Z"}},"outputs":[{"name":"stdout","text":"San Juan - Time Series Cross-Validation\nFold 1/5 | Train size:  152 | Test size: 156 | MAE: 26.6805\nFold 2/5 | Train size:  308 | Test size: 156 | MAE: 14.2060\nFold 3/5 | Train size:  464 | Test size: 156 | MAE: 6.1003\nFold 4/5 | Train size:  620 | Test size: 156 | MAE: 3.5632\nFold 5/5 | Train size:  776 | Test size: 156 | MAE: 8.7927\nCV Results: Mean MAE = 11.8685 ± 8.2045\nIquitos - Time Series Cross-Validation\nFold 1/5 | Train size:   86 | Test size:  86 | MAE: 5.0699\nFold 2/5 | Train size:  172 | Test size:  86 | MAE: 5.2896\nFold 3/5 | Train size:  258 | Test size:  86 | MAE: 3.5831\nFold 4/5 | Train size:  344 | Test size:  86 | MAE: 5.8947\nFold 5/5 | Train size:  430 | Test size:  86 | MAE: 3.8642\nCV Results: Mean MAE = 4.7403 ± 0.8775\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"## 9. Train Final Models on Full Data\n\nAfter validation, final models are trained using the complete historical dataset.\n\nSteps:\n\n- Apply missing value handling\n- Compute lag features\n- Apply log transformation to stabilize variance\n- Train CatBoost with MAE objective\n\nThese models are used for final forecasting.","metadata":{}},{"cell_type":"code","source":"def train_final_model(X_climate, y, city_name, params=None):\n   \n    #Train final production model on all available training data.\n    if params is None:\n        params = {\n            'loss_function': 'MAE',\n            'iterations': 500,\n            'depth': 6,\n            'learning_rate': 0.03,\n            'random_seed': 42,\n            'verbose': 100\n        }\n    \n    print(f\"\\nTraining final {city_name} model...\")\n    print(\"=\"*60)\n    \n    # Fill missing values\n    X_climate_filled = X_climate.ffill().bfill()\n    \n    # Add lag features\n    X_lagged = add_target_lags(X_climate_filled, y)\n    \n    # Drop NaN rows\n    valid_idx = X_lagged.dropna().index\n    X_final = X_lagged.loc[valid_idx]\n    y_final = y.loc[valid_idx]\n    \n    # Log transform\n    y_log = np.log1p(y_final)\n    \n    # Train\n    model = CatBoostRegressor(**params)\n    model.fit(X_final, y_log)\n    \n    print(f\"\\n✓ {city_name} model trained on {len(X_final)} samples\")\n    \n    return model, X_final.columns.tolist()\n\n# Train final models\nsj_final_model, sj_features = train_final_model(sj_X_climate, sj_y, \"San Juan\")\niq_final_model, iq_features = train_final_model(iq_X_climate, iq_y, \"Iquitos\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:49:43.669959Z","iopub.execute_input":"2026-02-11T16:49:43.670549Z","iopub.status.idle":"2026-02-11T16:49:45.617404Z","shell.execute_reply.started":"2026-02-11T16:49:43.670442Z","shell.execute_reply":"2026-02-11T16:49:45.616287Z"}},"outputs":[{"name":"stdout","text":"\nTraining final San Juan model...\n============================================================\n0:\tlearn: 0.7924395\ttotal: 4.44ms\tremaining: 2.21s\n100:\tlearn: 0.2830851\ttotal: 215ms\tremaining: 851ms\n200:\tlearn: 0.2486544\ttotal: 419ms\tremaining: 623ms\n300:\tlearn: 0.2260765\ttotal: 624ms\tremaining: 413ms\n400:\tlearn: 0.2065425\ttotal: 829ms\tremaining: 205ms\n499:\tlearn: 0.1931466\ttotal: 1.02s\tremaining: 0us\n\n✓ San Juan model trained on 932 samples\n\nTraining final Iquitos model...\n============================================================\n0:\tlearn: 0.8404581\ttotal: 2.19ms\tremaining: 1.09s\n100:\tlearn: 0.3785882\ttotal: 140ms\tremaining: 555ms\n200:\tlearn: 0.3218011\ttotal: 284ms\tremaining: 423ms\n300:\tlearn: 0.2811807\ttotal: 423ms\tremaining: 279ms\n400:\tlearn: 0.2504238\ttotal: 569ms\tremaining: 140ms\n499:\tlearn: 0.2217702\ttotal: 710ms\tremaining: 0us\n\n✓ Iquitos model trained on 516 samples\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## 10. Prepare Test Features\n\nTo generate predictions, test features must match the training feature structure.\n\nLag features for test data are computed using historical case counts from training.\n\nThis reflects real-world deployment where past observations are always available.","metadata":{}},{"cell_type":"code","source":"def prepare_test_features(test_X_climate, train_y, feature_list):\n    \"\"\"\n    Prepare test features with lag features using historical training data.\n    \n    Parameters:\n    -----------\n    test_X_climate : DataFrame - Climate features for test set\n    train_y : Series - Historical target values from training\n    feature_list : list - List of feature names expected by the model\n    \"\"\"\n    # Fill missing values\n    test_X_filled = test_X_climate.ffill().bfill()\n    \n    # Combine train and test targets for lag computation\n    # This is VALID because we're only using historical data\n    combined_y = train_y.copy()\n    \n    # Add lag features\n    test_X_lagged = add_target_lags(test_X_filled, combined_y)\n    \n    # Handle any remaining NaN\n    test_X_lagged = test_X_lagged.ffill().bfill()\n    \n    # Ensure same features as training\n    for col in feature_list:\n        if col not in test_X_lagged.columns:\n            test_X_lagged[col] = 0  # Add missing feature with 0\n    \n    test_X_final = test_X_lagged[feature_list]\n    \n    return test_X_final\n\n\ndef predict_test_set(test_features, train_y_sj, train_y_iq, \n                     model_sj, model_iq, feature_list_sj, feature_list_iq):\n\n    #Generate predictions for test set for separate cities\n    test_sj = test_features.loc[\"sj\"].copy()\n    test_iq = test_features.loc[\"iq\"].copy()\n    \n    # Drop date column\n    test_sj = test_sj.drop(\"week_start_date\", axis=1, errors='ignore')\n    test_iq = test_iq.drop(\"week_start_date\", axis=1, errors='ignore')\n    \n    # Extract climate features\n    test_sj_climate = test_sj[base_features].copy()\n    test_iq_climate = test_iq[base_features].copy()\n    \n    # Prepare features\n    test_sj_final = prepare_test_features(test_sj_climate, train_y_sj, feature_list_sj)\n    test_iq_final = prepare_test_features(test_iq_climate, train_y_iq, feature_list_iq)\n    \n    # Predict\n    sj_preds_log = model_sj.predict(test_sj_final)\n    iq_preds_log = model_iq.predict(test_iq_final)\n    \n    # Inverse transform and clip\n    sj_preds = np.expm1(sj_preds_log).clip(0).round().astype(int)\n    iq_preds = np.expm1(iq_preds_log).clip(0).round().astype(int)\n    \n    return sj_preds, iq_preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T16:49:45.619009Z","iopub.execute_input":"2026-02-11T16:49:45.619574Z","iopub.status.idle":"2026-02-11T16:49:45.630562Z","shell.execute_reply.started":"2026-02-11T16:49:45.619470Z","shell.execute_reply":"2026-02-11T16:49:45.628820Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## 11. Generate Predictions\n\nPredictions are generated by:\n\n1. Preparing features  \n2. Applying trained models  \n3. Inverting the log transformation  \n4. Enforcing non-negative case counts  \n\nOutputs are formatted according to competition requirements.","metadata":{}},{"cell_type":"markdown","source":"#### Create Submission File\n\nPredictions are saved into the required submission format:\n\n- City  \n- Year  \n- Week of year  \n- Predicted total cases  \n\nThe final CSV is ready for evaluation.","metadata":{}},{"cell_type":"code","source":"# Load test features by loading dataset again\ntest_features = pd.read_csv(\n    \"/kaggle/input/datasets/hardikthapar/deng123/DengAI_Test_Data_Features.csv\",\n    index_col=[0, 1, 2]\n)\n\n# Generates predictions\nsj_predictions, iq_predictions = predict_test_set(\n    test_features,\n    sj_y,\n    iq_y,\n    sj_final_model,\n    iq_final_model,\n    sj_features,\n    iq_features\n)\n\n# Creates submission file\nsubmission = pd.DataFrame({\n    'city': ['sj'] * len(sj_predictions) + ['iq'] * len(iq_predictions),\n    'year': test_features.index.get_level_values(1).tolist(),\n    'weekofyear': test_features.index.get_level_values(2).tolist(),\n    'total_cases': np.concatenate([sj_predictions, iq_predictions])\n})\n\nsubmission.to_csv('dengue_predictions.csv', index=False)\nprint(\" Predictions saved to dengue_predictions.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T17:03:53.607659Z","iopub.execute_input":"2026-02-11T17:03:53.607962Z","iopub.status.idle":"2026-02-11T17:03:53.664285Z","shell.execute_reply.started":"2026-02-11T17:03:53.607940Z","shell.execute_reply":"2026-02-11T17:03:53.663163Z"}},"outputs":[{"name":"stdout","text":" Predictions saved to dengue_predictions.csv\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"## 12. Feature Importance Analysis\n\nWe extract feature importance from CatBoost.\n\nThis helps verify:\n\n- Lag features contribute strongly  \n- Climate variables remain meaningful predictors  \n\nIt provides interpretability for the forecasting model.","metadata":{}},{"cell_type":"code","source":"def plot_feature_importance(model, feature_names, city_name):\n   \n    #Display feature importance from trained model.\n    importance = model.get_feature_importance()\n    feature_importance_df = pd.DataFrame({\n        'feature': feature_names,\n        'importance': importance\n    }).sort_values('importance', ascending=False)\n    \n    print(f\"\\n{city_name} - Top 10 Most Important Features:\")\n    print(feature_importance_df.head(10).to_string(index=False))\n    \n    return feature_importance_df\n\n# Display feature importance\nsj_importance = plot_feature_importance(sj_final_model, sj_features, \"San Juan\")\niq_importance = plot_feature_importance(iq_final_model, iq_features, \"Iquitos\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T17:03:14.369503Z","iopub.execute_input":"2026-02-11T17:03:14.369879Z","iopub.status.idle":"2026-02-11T17:03:14.388458Z","shell.execute_reply.started":"2026-02-11T17:03:14.369853Z","shell.execute_reply":"2026-02-11T17:03:14.387347Z"}},"outputs":[{"name":"stdout","text":"\nSan Juan - Top 10 Most Important Features:\n                              feature  importance\n                                lag_1   22.993459\n                       rolling_mean_4   15.855150\n                                lag_2   10.174249\n                       rolling_mean_8    9.683045\n                                lag_3    7.116145\n                                lag_4    6.043782\n                   station_avg_temp_c    5.752635\n                        rolling_std_4    5.519847\nreanalysis_specific_humidity_g_per_kg    5.209066\n                 precipitation_amt_mm    4.561265\n\nIquitos - Top 10 Most Important Features:\n                              feature  importance\n                                lag_1   20.023508\n                        rolling_std_4   11.421250\n                       rolling_mean_4   10.863390\n                                lag_2   10.118560\n                       rolling_mean_8    8.975203\n                   station_avg_temp_c    7.531172\n                 precipitation_amt_mm    6.248971\n          reanalysis_dew_point_temp_k    5.092045\nreanalysis_specific_humidity_g_per_kg    5.079975\n                   station_min_temp_c    4.998273\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"## 13. Final Validation Summary\n\nThis model was developed under strict time-series constraints.\n\nKey safeguards:\n\n- No shuffling  \n- No future data usage  \n- Lag features computed correctly  \n- Cross-validation done chronologically  \n- Log transformation applied consistently  \n\nCross-validation MAE reflects realistic forecasting performance.\n\nThe pipeline is reproducible and suitable for production-style deployment.","metadata":{}},{"cell_type":"code","source":"print(\"\\n Notes: :\")\nprint(\"  • Lag features are computed per-fold during CV\")\nprint(\"  • Missing values filled per-fold\")\nprint(\"  • TimeSeriesSplit is used (no random shuffling is done)\")\nprint(\"  • Test data is never used in training\")\n\nprint(\"\\n Maximum Achieved Performance:\")\nprint(f\"  • San Juan CV MAE: {sj_mean_mae:.4f}\")\nprint(f\"  • Iquitos CV MAE: {iq_mean_mae:.4f}\")\n\nprint(\"\\n Feature's Engineered:\")\nprint(f\"  • {len(base_features)} climate based features\")\nprint(\"  • Target lags: 1, 2, 3, 4 weeks\")\nprint(\"  • Rolling means: 4, 8 weeks\")\nprint(\"  • Rolling std: 4 weeks\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T17:01:56.759700Z","iopub.execute_input":"2026-02-11T17:01:56.760092Z","iopub.status.idle":"2026-02-11T17:01:56.767727Z","shell.execute_reply.started":"2026-02-11T17:01:56.760064Z","shell.execute_reply":"2026-02-11T17:01:56.766612Z"}},"outputs":[{"name":"stdout","text":"\n Notes: :\n  • Lag features are computed per-fold during CV\n  • Missing values filled per-fold\n  • TimeSeriesSplit is used (no random shuffling is done)\n  • Test data is never used in training\n\n Maximum Achieved Performance:\n  • San Juan CV MAE: 11.8685\n  • Iquitos CV MAE: 4.7403\n\n Feature's Engineered:\n  • 5 climate based features\n  • Target lags: 1, 2, 3, 4 weeks\n  • Rolling means: 4, 8 weeks\n  • Rolling std: 4 weeks\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"## Conclusions\n\nThis project investigates an end-to-end time-series based forecasting for weekly dengue case prediction in San Juan and Iquitos.\n\nThe workflow was as follows:\n\n- Structured exploratory data analysis\n- Careful feature selection, grounded in climate relevance\n- Temporal lag engineering to capture the outbreak dynamics\n- Log transformation for variance stabilization\n- TimeSeriesSplit for strict chronological cross-validation\n- Modeling to be done separately for each city as epidemiological behaviors may differ\n- Data validation measures to prevent data leakage\n\nThis pipeline is a realistic forecasting setting validating that:\n- No shuffling of time-series data is done\n- No forward-looking bias during training happens\n- Lag features computed correctly\n- Evaluation is aligned with the competition metric (MAE) as per the guidelines\n\n _We iteratively refined from baseline linear methods_ [LinearRegression, RandomForests] _to a temporal-aware, robust gradient boosting framework_ CatBoostRegressor.\n\n\nMore importantly, the process stated:\n- Methodological correctness\n- True acknowledgment\n\nReproducibility Interpretability of this notebook represents a complete, production-style time-series modeling pipeline suitable for epidemiological forecasting tasks. For me it was not just about improving MAE; rather, the focus was on building a sound and defensible technical forecasting system :)","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}